from knowledge_model import KnowledgeModel
import re
from collections import Counter
from llm_engine import LocalLLMEngine, HybridResponseGenerator

class AIAgent:
    def __init__(self, document_processor, model_path="modelos/model.pkl", use_llm=True):
        self.document_processor = document_processor
        self.knowledge_model = KnowledgeModel.load(model_path) or KnowledgeModel(model_path)
        
        # üß† Inicializa LLM local para respostas mais naturais
        self.llm_engine = LocalLLMEngine(enabled=use_llm)
        self.hybrid_generator = HybridResponseGenerator(self.llm_engine)
        self.use_llm = use_llm and self.llm_engine.is_available()
        
        if self.use_llm:
            print("‚úÖ Chat com LLM local ativado - respostas naturais!")
        else:
            print("üìù Chat em modo tradicional - sem LLM")

        self.greetings = {
            "oi": "Ol√°! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "ola": "Ol√°! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "ol√°": "Ol√°! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "opa": "Opa! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "hey": "Hey! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "e ai": "E a√≠! Sou o agente de IA da melhoria cont√≠nua. Como posso ajud√°-lo?",
            "tudo bem": "Tudo bem sim! Sou o agente de IA da melhoria cont√≠nua. Em que posso ajud√°-lo?",
            "obrigado": "De nada! Fico feliz em ajudar.",
            "valeu": "De nada! Fico feliz em ajudar.",
            "tchau": "At√© logo!"
        }
        
        # Respostas para perguntas sobre identidade/pessoais
        self.identity_patterns = {
            r"(quem|qual) (?:√©|sou) (?:voc√™|vc|seu nome)": "Sou um assistente de IA criado para ajudar com informa√ß√µes. Estou aqui para responder suas d√∫vidas sobre os t√≥picos da minha base de conhecimento!",
            r"(qual|quem) (?:s√£o|sao|somos) (?:os )?chefes": "Essa informa√ß√£o espec√≠fica n√£o est√° documentada para mim, mas voc√™ pode verificar o organograma da empresa ou conversar com seu gestor direto!",
            r"(quem|qual) (?:√©|s√£o) (?:voc√™|vc|seu criador|seu desenvolvedor)": "Fui desenvolvido como um assistente inteligente para esta empresa. Minha fun√ß√£o √© tornar o acesso √† informa√ß√£o mais r√°pido e f√°cil!",
            r"(qual|quem) √© (?:meu|seu) (?:chefe|supervisor|gerente)": "Essa informa√ß√£o √© mais adequada para ser obtida no sistema de RH ou com seu gestor direto. Posso ajudar com outras informa√ß√µes?",
            r"(como|aonde) (?:posso )?(?:te )?encontrar|(?:qual|onde) (?:√©|fica) (?:voc√™|vc)": "Estou aqui neste chat, dispon√≠vel 24/7 para ajudar! Voc√™ tamb√©m pode consultar a documenta√ß√£o armazenada em minha base de conhecimento.",
            r"(qual|quem) (?:trabalha|trabalham) (?:com|aqui)": "√ìtima pergunta! Mas para informa√ß√µes sobre membros da equipe, recomendo consultar o sistema interno ou checar o diret√≥rio da empresa.",
        }

    def _is_greeting(self, message):
        msg = re.sub(r'[^\w\s]', '', message.lower().strip())
        return any(g in msg for g in self.greetings.keys())

    def _respond_greeting(self, message):
        msg = re.sub(r'[^\w\s]', '', message.lower().strip())
        for g, r in self.greetings.items():
            if g in msg:
                return r
        return "Ol√°! Como posso ajud√°-lo?"
    
    def _is_identity_question(self, message):
        """Detecta perguntas sobre identidade, pessoais ou fora do escopo"""
        msg = message.lower()
        return any(re.search(pattern, msg) for pattern in self.identity_patterns.keys())
    
    def _respond_identity_question(self, message):
        """Responde perguntas sobre identidade/pessoais de forma humanizada"""
        msg = message.lower()
        for pattern, response in self.identity_patterns.items():
            if re.search(pattern, msg):
                return response
        # Resposta padr√£o se n√£o encontrar padr√£o espec√≠fico
        return "Essa √© uma √≥tima pergunta! Infelizmente, essa informa√ß√£o n√£o est√° na minha base de conhecimento, mas posso ajudar com outras d√∫vidas!"

    def _keywords(self, text):
        words = re.findall(r'\w+', text.lower())
        stop = {"o","a","os","as","de","do","da","dos","das","em","para","por","com","um","uma","e","ou","n√£o","que","se","na","no","nas","nos"}
        return [w for w in words if w not in stop and len(w) > 3]

    def _best_sentences(self, docs, query):
        keywords = self._keywords(query)
        sentences = []
        for _, _, doc in docs:
            parts = re.split(r'[.!?]+', doc)
            for s in parts:
                s = s.strip()
                if len(s) < 40:
                    continue
                score = sum(1 for k in keywords if k in s.lower())
                if score > 0:
                    sentences.append((score, s))
        sentences.sort(key=lambda x: x[0], reverse=True)
        return [s for _, s in sentences[:8]]

    def train_and_save_model(self):
        docs = [c["text"] for c in self.document_processor.get_chunks()]
        self.knowledge_model.train(docs)
        self.knowledge_model.save()
        print(f"‚úì Modelo treinado com {len(docs)} chunks")

    def _reformat_response(self, text):
        """Reformula a resposta para ser mais natural e fluida"""
        # 1. Quebra em senten√ßas
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 15]
        
        if not sentences:
            return text
        
        # 2. Remove duplicatas mantendo ordem
        seen = set()
        unique_sents = []
        for s in sentences:
            s_normalized = s.lower().strip()
            if s_normalized not in seen:
                seen.add(s_normalized)
                unique_sents.append(s)
        
        if not unique_sents:
            return text
        
        # 3. Ordena por tamanho (prioriza senten√ßas mais informativas)
        sorted_sents = sorted(unique_sents, key=lambda x: len(x), reverse=True)[:5]
        
        # 4. Conectivos naturais para fluir melhor entre senten√ßas
        connectors = [
            "Al√©m disso,",
            "Vale mencionar que",
            "√â importante destacar que", 
            "Somando a isso,",
            "Tamb√©m podemos notar que",
            "Com rela√ß√£o a isso,",
            "Nesse contexto,"
        ]
        
        # 5. Constr√≥i resposta mais natural
        response = sorted_sents[0] + "."
        
        for i, sent in enumerate(sorted_sents[1:], 1):
            connector = connectors[i % len(connectors)]
            response += f" {connector} {sent.lower() if sent[0].isupper() else sent}."
        
        return response

    def chat(self, message):
        # 1. Verifica se √© sauda√ß√£o - SEMPRE usa resposta pr√©-definida
        if self._is_greeting(message):
            return self._respond_greeting(message)
        
        # 2. Verifica se √© pergunta sobre identidade/pessoal
        if self._is_identity_question(message):
            return self._respond_identity_question(message)

        # 3. Busca nos documentos com Word2Vec + TF-IDF
        results = self.knowledge_model.query(message, top_k=5)
        if not results:
            return "Desculpa, n√£o encontrei informa√ß√µes suficientes nos documentos."

        # 4. Extrai documentos relevantes
        relevant_docs = []
        for idx, score, doc in results:
            if score > 0.1:  # Apenas documentos com relev√¢ncia m√≠nima
                relevant_docs.append(doc.strip())
        
        if not relevant_docs:
            return "N√£o encontrei um trecho claro nos documentos. Tente ser mais espec√≠fico."
        
        # 5. üß† MODO H√çBRIDO: Tenta LLM primeiro, depois fallback tradicional
        if self.use_llm:
            # LLM reformula resposta baseado nos documentos encontrados
            response, used_llm = self.hybrid_generator.generate_response(
                message, 
                relevant_docs, 
                use_llm=True
            )
            if used_llm:
                return response
        
        # Fallback: Modo tradicional (extra√ß√£o de senten√ßas + reformula√ß√£o)
        best_sents = self._best_sentences(results, message)
        if best_sents:
            raw_response = " ".join(best_sents)
            return self._reformat_response(raw_response)
        
        # √öltimo recurso: retorna docs concatenados e reformatados
        raw_response = "\n\n".join(relevant_docs[:2])
        return self._reformat_response(raw_response)
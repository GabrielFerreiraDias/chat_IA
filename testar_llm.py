"""
TESTE R√ÅPIDO - Sistema Completo com LLM Local
==============================================

Este script testa:
1. Carregamento do modelo treinado
2. Busca com Word2Vec + TF-IDF
3. LLM local (se dispon√≠vel)
4. Sistema h√≠brido de respostas
"""

import os
import sys

def test_basic_imports():
    """Testa imports b√°sicos"""
    print("\n" + "="*60)
    print("1Ô∏è‚É£ TESTANDO IMPORTS B√ÅSICOS")
    print("="*60)
    
    try:
        from knowledge_model import KnowledgeModel
        print("‚úÖ KnowledgeModel importado")
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        return False
    
    try:
        from agent import AIAgent
        print("‚úÖ AIAgent importado")
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        return False
    
    try:
        from llm_engine import LocalLLMEngine, HybridResponseGenerator
        print("‚úÖ LLM Engine importado")
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        return False
    
    return True

def test_model_loading():
    """Testa carregamento do modelo treinado"""
    print("\n" + "="*60)
    print("2Ô∏è‚É£ TESTANDO CARREGAMENTO DO MODELO")
    print("="*60)
    
    try:
        from knowledge_model import KnowledgeModel
        
        if not os.path.exists("modelos/model.pkl"):
            print("‚ö†Ô∏è Modelo n√£o encontrado!")
            print("üí° Execute: python train_model.py")
            return False
        
        model = KnowledgeModel.load("modelos/model.pkl")
        print("‚úÖ Modelo carregado com sucesso")
        
        # Verifica componentes
        if hasattr(model, 'nlp_engine'):
            print("‚úÖ NLP Engine presente")
        else:
            print("‚ö†Ô∏è NLP Engine n√£o encontrado (treinar novamente?)")
        
        if hasattr(model, 'vectorizer'):
            print("‚úÖ TF-IDF Vectorizer presente")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo: {e}")
        return False

def test_llm_availability():
    """Testa disponibilidade do LLM local"""
    print("\n" + "="*60)
    print("3Ô∏è‚É£ TESTANDO LLM LOCAL")
    print("="*60)
    
    try:
        from llm_engine import LocalLLMEngine
        
        # Verifica bibliotecas LLM dispon√≠veis
        has_llama_cpp = False
        has_ctransformers = False
        
        try:
            import llama_cpp
            print("‚úÖ llama-cpp-python instalado")
            has_llama_cpp = True
        except ImportError:
            print("‚ö†Ô∏è llama-cpp-python N√ÉO instalado")
        
        try:
            import ctransformers
            print("‚úÖ ctransformers instalado")
            has_ctransformers = True
        except ImportError:
            print("‚ö†Ô∏è ctransformers N√ÉO instalado")
        
        if not has_llama_cpp and not has_ctransformers:
            print("\n‚ùå Nenhuma biblioteca LLM instalada!")
            print("üí° Instale uma das op√ß√µes:")
            print("   - pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu")
            print("   - pip install ctransformers")
            return False
        
        # Verifica se h√° modelo .gguf
        model_dir = "modelos"
        gguf_files = []
        if os.path.exists(model_dir):
            gguf_files = [f for f in os.listdir(model_dir) if f.endswith('.gguf')]
        
        if gguf_files:
            print(f"‚úÖ Modelo(s) .gguf encontrado(s): {', '.join(gguf_files)}")
        else:
            print("‚ö†Ô∏è Nenhum modelo .gguf encontrado em modelos/")
            print("üí° Baixe Phi-3-mini: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf")
            return False
        
        # Tenta inicializar LLM
        print("\n‚è≥ Tentando carregar LLM (pode levar alguns segundos)...")
        llm = LocalLLMEngine(enabled=True)
        
        if llm.is_available():
            print("‚úÖ LLM local DISPON√çVEL e FUNCIONANDO!")
            return True
        else:
            print("‚ö†Ô∏è LLM local n√£o dispon√≠vel")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro ao testar LLM: {e}")
        return False

def test_query_without_llm():
    """Testa query sem LLM (modo tradicional)"""
    print("\n" + "="*60)
    print("4Ô∏è‚É£ TESTANDO QUERY SEM LLM (Tradicional)")
    print("="*60)
    
    try:
        from document_processador import DocumentProcessor
        from agent import AIAgent
        
        dp = DocumentProcessor("aprendizado")
        dp.process_all_documents()
        
        # Cria agente SEM LLM
        agent = AIAgent(dp, use_llm=False)
        
        # Testa query
        query = "Como funciona o controle de documentos?"
        print(f"\nüìù Query: {query}")
        
        response = agent.chat(query)
        print(f"\nü§ñ Resposta (sem LLM):\n{response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_query_with_llm():
    """Testa query COM LLM (modo h√≠brido)"""
    print("\n" + "="*60)
    print("5Ô∏è‚É£ TESTANDO QUERY COM LLM (H√≠brido)")
    print("="*60)
    
    try:
        from document_processador import DocumentProcessor
        from agent import AIAgent
        
        dp = DocumentProcessor("aprendizado")
        dp.process_all_documents()
        
        # Cria agente COM LLM
        agent = AIAgent(dp, use_llm=True)
        
        if not agent.use_llm:
            print("‚ö†Ô∏è LLM n√£o est√° dispon√≠vel")
            print("üí° Verifique etapa anterior (3Ô∏è‚É£)")
            return False
        
        # Testa query
        query = "Como funciona o controle de documentos?"
        print(f"\nüìù Query: {query}")
        
        print("\n‚è≥ Gerando resposta com LLM (pode levar 1-3 segundos)...")
        response = agent.chat(query)
        print(f"\nüß† Resposta (com LLM):\n{response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Executa todos os testes"""
    print("\n" + "="*60)
    print("üß™ TESTE COMPLETO DO SISTEMA COM LLM LOCAL")
    print("="*60)
    
    results = {}
    
    # Teste 1: Imports
    results['imports'] = test_basic_imports()
    
    # Teste 2: Modelo
    results['model'] = test_model_loading()
    
    # Teste 3: LLM
    results['llm'] = test_llm_availability()
    
    # Teste 4: Query tradicional
    if results['model']:
        results['query_traditional'] = test_query_without_llm()
    else:
        results['query_traditional'] = False
        print("\n‚ö†Ô∏è Pulando teste de query tradicional (modelo n√£o carregado)")
    
    # Teste 5: Query com LLM
    if results['model'] and results['llm']:
        results['query_llm'] = test_query_with_llm()
    else:
        results['query_llm'] = False
        print("\n‚ö†Ô∏è Pulando teste de query com LLM (pr√©-requisitos n√£o atendidos)")
    
    # Resumo final
    print("\n" + "="*60)
    print("üìä RESUMO DOS TESTES")
    print("="*60)
    
    for test_name, passed in results.items():
        icon = "‚úÖ" if passed else "‚ùå"
        print(f"{icon} {test_name.upper()}: {'PASSOU' if passed else 'FALHOU'}")
    
    total = len(results)
    passed = sum(results.values())
    
    print("\n" + "="*60)
    print(f"üìà RESULTADO FINAL: {passed}/{total} testes passaram")
    print("="*60)
    
    if passed == total:
        print("\nüéâ TUDO FUNCIONANDO PERFEITAMENTE!")
        print("‚úÖ Sistema pronto para produ√ß√£o com LLM local")
        print("\nüí° Pr√≥ximo passo: python app.py")
    elif results.get('query_traditional'):
        print("\n‚úÖ Sistema FUNCIONAL (modo tradicional)")
        print("‚ö†Ô∏è LLM local n√£o dispon√≠vel, mas tudo funciona normalmente")
        print("\nüí° Para ativar LLM:")
        print("   1. Certifique-se que tem uma biblioteca LLM:")
        print("      - ctransformers (J√Å instalado? ‚úÖ)")
        print("      - OU llama-cpp-python")
        print("   2. Baixe um modelo .gguf:")
        print("      - TinyLlama (800MB): https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF")
        print("      - Phi-3-mini (2GB): https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf")
        print("   3. Coloque o arquivo .gguf em modelos/")
        print("   4. Execute este script novamente")
        print("\nüìö Guia completo: BAIXAR_MODELO.md")
    else:
        print("\n‚ùå Sistema com problemas")
        print("üí° Verifique os testes falhados acima")
        print("üìö Consulte GUIA_LLM_LOCAL.md para ajuda")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
